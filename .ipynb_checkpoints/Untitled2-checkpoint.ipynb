{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../projectInfo.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ccfe4a749eea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-ccfe4a749eea>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../projectInfo.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0msave_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vocabulary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mloglikelihoods\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mguided_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloglikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtopic_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTOPIC_NUMS_TUPLE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-ccfe4a749eea>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;36m5.\u001b[0m \u001b[0mconvert\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mto\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \"\"\"\n\u001b[0;32m--> 228\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m         \u001b[0mdata_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../projectInfo.json'"
     ]
    }
   ],
   "source": [
    "import guidedlda\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "TOPIC_NUMS_TUPLE = (10, 30, 50, 100, 150, 200, 300)\n",
    "ITERATION_NUMS = 100\n",
    "TOP_K_WORDS = 25\n",
    "STOP_WORDS_FILE_PATH = 'stop_word_list.txt'\n",
    "\n",
    "def main():\n",
    "    X, word2id, vocab = load_data('../projectInfo.json')\n",
    "    save_file(vocab, '', 'vocabulary')\n",
    "    loglikelihoods = [guided_analysis(X, word2id, vocab, topic_num).loglikelihood() for topic_num in TOPIC_NUMS_TUPLE]\n",
    "    plt.plot(list(TOPIC_NUMS_TUPLE), loglikelihoods)\n",
    "    plt.show()\n",
    "    plt.savefig('loglikelihood_topicnum.png')\n",
    "\n",
    "\n",
    "\n",
    "def guided_analysis(X, word2id, vocab, topic_num, n_top_words=TOP_K_WORDS):\n",
    "    \"\"\"\n",
    "    Guided Analysis on the given dtm\n",
    "    \"\"\"\n",
    "    model = guidedlda.GuidedLDA(n_topics=topic_num, n_iter=ITERATION_NUMS, random_state=7, refresh=20)\n",
    "    model.fit(X, seed_topics=load_seed_topics(word2id), seed_confidence = 0.25)\n",
    "    retrieve_words_from(model, vocab, topic_num, n_top_words)\n",
    "    return model\n",
    "\n",
    "\n",
    "def non_guided_analysis(X, vocab, topic_num, n_top_words=TOP_K_WORDS):\n",
    "    \"\"\"\n",
    "    Non_guided Analysis on the given dtm\n",
    "    \"\"\"\n",
    "    model = guidedlda.GuidedLDA(n_topics= topic_num, n_iter=ITERATION_NUMS, random_state=7, refresh=20)\n",
    "    model.fit(X)\n",
    "    retrieve_words_from(model, vocab, topic_num, n_top_words)\n",
    "    dt_matrix(model, 10, topic_num)\n",
    "    tw_matrix(model, 10, topic_num)\n",
    "    # unique_words(model, vocab, 10, topic_num)\n",
    "\n",
    "\n",
    "def dt_matrix(model, n_top_docs, topic_num):\n",
    "    \"\"\"\n",
    "    Export the document - topic matrix\n",
    "    \"\"\"\n",
    "    doc_topic = model.doc_topic_\n",
    "    result = []\n",
    "    for i, docs_dist in enumerate(doc_topic):\n",
    "        doc_topic_assignment = np.sort(docs_dist)[: -(n_top_docs + 1): -1]\n",
    "        result.append('Document {} : {}'.format(i, ','.join(map(str, doc_topic_assignment))))\n",
    "    save_file(result, topic_num, 'document_topic_matrix')\n",
    "\n",
    "\n",
    "def tw_matrix(model, n_top_words, topic_num):\n",
    "    \"\"\"\n",
    "    Export the topic- word matrix\n",
    "    \"\"\"\n",
    "    topic_word = model.topic_word_\n",
    "    result = []\n",
    "    for i, word_dist in enumerate(topic_word):\n",
    "        topic_word_assignment = np.sort(word_dist)[: -(n_top_words + 1): - 1]\n",
    "        result.append('Topic {} : {}'.format(i, ','.join(map(str, topic_word_assignment))))\n",
    "    save_file(result, topic_num, 'topic_word_matrix')\n",
    "\n",
    "\n",
    "def unique_words(model, vocab, n_top_words, topic_num):\n",
    "    \"\"\"\n",
    "    Find out the unique words for the topics\n",
    "    \"\"\"\n",
    "    word_topic = model.word_topic_\n",
    "    result = []\n",
    "    for i, topic_dist in enumerate(word_topic):\n",
    "        beta_dist = np.array(list(map(lambda x: float(x) / (1 - x), topic_dist)))\n",
    "        # pick n_top beta for the words in the topic\n",
    "        sorted_index = np.argsort(beta_dist)[: -(n_top_words + 1): - 1]\n",
    "        sorted_beta = beta_dist[sorted_index]\n",
    "        result.append(('{} : {}'.format(vocab[i], ','.join(list(map(lambda x, y: str(x) + '(' + str(y) + ')', sorted_index, sorted_beta))))))\n",
    "    save_file(result, topic_num, 'word_uniqueness_matrix')\n",
    "\n",
    "\n",
    "def retrieve_words_from(model, vocab, topic_num, n_top_words):\n",
    "    \"\"\"\n",
    "    Retrieve the top k topics\n",
    "    \"\"\"\n",
    "    topic_word = model.topic_word_\n",
    "    result = []\n",
    "    for i, topic_dist in enumerate(topic_word):\n",
    "        word_index = np.argsort(topic_dist)[:-(n_top_words + 1): -1]\n",
    "        topic_words = np.array(vocab)[word_index]\n",
    "        topic_words_assignment = topic_dist[word_index]\n",
    "        result.append('Topic {} : {}'.format(i, ','.join(list(map(lambda x, y : str(x)+ '(' + str(y) + ')', topic_words, format_result(topic_words_assignment))))))\n",
    "    save_file(result, topic_num, 'topic_words_matrix')\n",
    "\n",
    "def format_result(data_list):\n",
    "    return [\"{0:.2f}\".format(i) for i in data_list]\n",
    "\n",
    "def load_seed_topics(word2id):\n",
    "    \"\"\"\n",
    "    Construct the seeds_topic dictionary\n",
    "    :param word2id:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    seed_topic_list = {\n",
    "        'team': ['crew', 'team', 'seriously', 'experienced', 'collaborator', 'programmer',\n",
    "                 'programmer', 'marketing', 'brother', 'leadership', 'leader', 'developer',\n",
    "                 'designer', 'leadership', 'post', 'artist', 'writer', 'director',\n",
    "                 'research', ' researcher', 'researchers', 'develop', 'cinematography'],\n",
    "        'product': ['characteristic', 'theme', 'role', 'evolution', 'feature', 'story',\n",
    "                    'storytelling', 'attribute', 'design', 'expansion', 'interactive',\n",
    "                    ' aspect', 'aspects', 'language', 'gameplay', 'gear', 'ammunition',\n",
    "                    'potion', 'experience', 'rpg', 'level', 'venture','activity',\n",
    "                    'adventure', 'adventurer', 'character', 'system', 'job', 'pack'],\n",
    "        'motivation': ['reason', 'inspired', 'suggest', 'fun', 'support', 'exciting', 'opportunity',\n",
    "                       'idea', 'excited', 'confident', 'feedback', 'ontrack'],\n",
    "        'rewards': ['goals', 'goal', 'pledge', 'access', 'reward', 'level', 'rewards', 'tier', 'appreciate',\n",
    "                    'appreciation', 'offer', 'bonus', 'promotion', 'price', 'share', 'bundle']\n",
    "    }\n",
    "    seed_topics = {}\n",
    "    for tid, seeds in enumerate(seed_topic_list.values()):\n",
    "        for word in seeds:\n",
    "            lower_word = word.lower()\n",
    "            if lower_word in word2id.keys():\n",
    "                seed_topics[word2id[lower_word]] = tid\n",
    "    return seed_topics\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    get part of speech from treebank ag\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "def load_stopwords(filepath):\n",
    "    \"\"\"\n",
    "    Load the stop words\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    with open(filepath) as fp:\n",
    "        for line in fp:\n",
    "            stop_words.add(line.rstrip('\\n'))\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "def filter_words(tokens, stop_words):\n",
    "    \"\"\"\n",
    "    filter the word by nltk stopwords and length\n",
    "    \"\"\"\n",
    "    return [w for w in tokens if w not in stop_words and len(w) > 3]\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"\\S*@\\S*\", \" \", text)  # remove email address\n",
    "    text = re.sub(r\"((:?http|https)://)?[-./?:@_=#\\w]+\\.(?:[a-zA-Z]){2,6}(?:[-\\w.&/?:@_=#()])*\", \" \", text)  # remove urls\n",
    "    text = re.sub(r\"[-!?=~|#$+%*&@:/(){}\\[\\],\\\"'._]\", \" \", text)  # remove punctuations\n",
    "    text = re.sub(r\"\\d+\", \" \", text) # remove digits\n",
    "    text = re.sub(r\"\\b(\\w)\\1+\\b\", \" \",text) # remove meaningless word composed\n",
    "\n",
    "    # Facebook account name\n",
    "    # Instagram account name\n",
    "    # Tumblr account name\n",
    "    # Word separated by &nbsp sticks together\n",
    "    return text\n",
    "\n",
    "\n",
    "def nltk_lemmatize(campaign_list):\n",
    "    \"\"\"\n",
    "    Return the campaign list after being lemmatized\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    wordnet_lemmatizer = nltk.WordNetLemmatizer()\n",
    "    stop_words = load_stopwords(STOP_WORDS_FILE_PATH)\n",
    "    for campaign in campaign_list:\n",
    "        token_list = []\n",
    "        tokens = filter_words(nltk.word_tokenize(campaign, language='english'), stop_words)\n",
    "        pos_tags = nltk.pos_tag(tokens)\n",
    "        for word, tag in pos_tags:\n",
    "            token_list.append(wordnet_lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag)))\n",
    "        result.append(\" \".join(token_list))\n",
    "    return result\n",
    "\n",
    "\n",
    "def spacy_lemmatize(campaign_list):\n",
    "    \"\"\"\n",
    "    Return the campaign list after being lemmatized by spacy\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    nlp = spacy.load('en', disable = ['parser', 'ner'])\n",
    "    for campaign in campaign_list:\n",
    "        doc = nlp(campaign)\n",
    "        result.append(\" \".join([token.lemma_ for token in doc if token.lemma_ != '-PRON-']))\n",
    "    return result\n",
    "\n",
    "\n",
    "def save_file(data, topic_num, filename):\n",
    "    with open('%s_%s.txt' % (str(topic_num), filename), 'w') as fp:\n",
    "        for item in data:\n",
    "            fp.write(item + \"\\n\")\n",
    "        fp.close()\n",
    "        \n",
    "def pretty_print(data, title):\n",
    "    print (\"------------- %s ---------------\")\n",
    "    for item in data:\n",
    "        print(item + \"\\n\")\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    1. Load the data from file\n",
    "    2. clean the text\n",
    "    3. lemmatize the text\n",
    "    4. extract the vocabulary from the documents\n",
    "    5. convert the data to document - term matrix\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as fp:\n",
    "        data_list = list(json.load(fp))\n",
    "        fp.close()\n",
    "    if data_list:\n",
    "        campaign_list = list(filter(lambda x : len(x) > 0, nltk_lemmatize([clean_text((\" \".join(item['ProjectCampaign'])).lower()) for item in data_list])))\n",
    "        save_file(campaign_list, '', 'campaign')\n",
    "        vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 1),\n",
    "                                     lowercase=True, analyzer='word')\n",
    "        X = vectorizer.fit_transform(campaign_list).toarray()\n",
    "        word2id = vectorizer.vocabulary_\n",
    "        vocab = vectorizer.get_feature_names()\n",
    "        return X, word2id, vocab\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
